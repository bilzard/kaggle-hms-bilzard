hydra:
  run:
    dir: ${env.working_dir}/${job_name}/${exp_name}/fold_${fold}/seed_${seed}
  sweep:
    dir: ${env.working_dir}/multirun/${job_name}
    subdir: ${hydra.job.override_dirname}
  job:
    name: ${job_name}
    chdir: true
    config:
      override_dirname:
        exclude_keys:
          - job_name
          - verbose
          - debug
          - description
          - wandb.project
          - wandb.mode
          - dry_run
          - trainer.save_last
          - trainer.save_best
          - no_eval

defaults:
  - env: local
  - preprocess: with_cqf
  - split: gkfold5
  - dev: small
  - trainer/optimizer: adamw
  - architecture/model/sample_collator: simple_collator
  - architecture/model/feature_extractor: wave2spec_128x32
  - architecture/model/consistency_regularizer: identity
  - architecture/model/encoder: timm_encoder
  - architecture/model/decoder: pick_last
  - architecture/model/sample_aggregator: mean_aggregator
  - architecture/model/feature_processor: identity
  - architecture/model/head: simple_head
  - _self_

trainer:
  trainer_class:
    _target_: src.trainer.Trainer
  epochs: 4
  lr: 1e-3
  lr_adjustments: []
  weight_decay: 0.0
  batch_size: 32
  save_last: true
  save_best: false
  duration: 2048
  no_decay_bias_params: false
  val:
    batch_size: 32
    duration: ${trainer.duration}
    stride: 1024
    aggregation_fn: mean
    agg_policy: per_eeg_weighted
    seed: 123
    weight_exponent: 0.0
    min_weight: 0.3
    only_use_sp_center: false
  scheduler:
    warmup_ratio: 0.0
  data:
    target_key: label
    pred_key: pred
    weight_key: weight
    input_keys: [eeg, cqf]
    sampler:
      enabled: false
      num_samples_per_epoch: 17_280
      sample_weight:
        hq-seizure: 0.837
        hq-lpd: 1.076
        hq-gpd: 0.696
        hq-lrda: 0.726
        hq-grda: 0.691
        hq-other: 1.130
        lq-seizure: 0.106
        lq-lpd: 1.645
        lq-gpd: 1.509
        lq-lrda: 0.494
        lq-grda: 0.317
        lq-other: 2.303
        vlq-seizure: 4.929
        vlq-lpd: 4.758
        vlq-gpd: 4.567
        vlq-lrda: 12.724
        vlq-grda: 6.672
        vlq-other: 3.921
  transform: null
  num_samples_per_eeg: 1
  log_file_name: train_pipeline.txt
  random_seed_offset: 127458
  train_dataset:
    _target_: src.dataset.eeg.UniformSamplingEegDataset
  valid_dataset:
    _target_: src.dataset.eeg.SlidingWindowEegDataset
  label:
    diversity_power: 0.0
    population_power: 1.0
    max_votes: 28
    weight_key: [weight]
    label_postfix: [_prob]
    only_use_sp_center: false
    min_weight: 0.0
    schedule:
      weight_exponent:
        schedule_start_epoch: 0
        target_epoch: 0
        initial_value: 1.0
        target_value: 1.0
      min_weight:
        schedule_start_epoch: 0
        target_epoch: 0
        initial_value: 0.0
        target_value: 0.0
      max_weight:
        schedule_start_epoch: 0
        target_epoch: 0
        initial_value: 1.0
        target_value: 1.0
  class_weights: [1.151, 0.706, 1.039, 1.681, 1.17, 0.253]
  class_weight_exponent: 0.0
  use_loss_weights: true
  distillation:
    teacher_exp_name: null
    target_epochs: 0
    target_forget_rate: 1.0
    use_loss_weights: false
  contrastive:
    lambd: 0.0
  loss_weight:
    norm_policy: relative
    global_mean: 0.227
  pseudo_label:
    ensemble_entity_name: null
    weight: 0.5
    injection_mode: replace
    min_weight: 0.3
  aux_loss:
    lambd: 0.0
    is_binary: true
    binary_threshold: 0.3
    freeze_epoch: .inf
  feature_extractor_freeze_epoch: .inf
  pretrained_weight:
    exp_name: null
    seed: null
    model_choice: last

architecture:
  model:
    eeg_pre_adapter:
      _target_: src.model.eeg_adapter.Identity
    augmentation:
      _target_: src.model.augmentation.Identity
    spec_transform: null
    adapters:
      - _target_: src.model.adapter.WeightedMeanStackingAggregator
      - _target_: src.model.adapter.ResizeTransform
        scale_factor: [2.0, 8.0]
    bg_adapters: []
    merger: null
    encoder:
      name: efficientnet_b0
    post_adapter:
      _target_: src.model.post_adapter.IdentityPostAdapter
  model_class:
    _target_: src.model.hms_model.HmsModel
  model_checker:
    _target_: src.model.hms_model.hms_model.check_model
  in_channels: 5
  in_channels_eeg: -1
  in_channels_spec: -1
  out_channels: 6
  recover_dual: false
  use_lr_feature: true
  use_similarity_feature: false
  hidden_dim: 64
  input_mask: false
  use_bg_spec: false
  lr_mapping_type: identity
  spec_cropped_duration: 256
  bg_spec_mask_value: 1.0

infer:
  batch_size: ${env.infer_batch_size}
  model_choice: last
  log_name: infer_pipeline.txt
  tta_iterations: 1
  tta: null
  test_dataset:
    _target_: src.dataset.eeg.PerEegSubsampleDataset
    num_samples_per_eeg: 1
    duration: ${trainer.duration}

wandb:
  project: kaggle-hms
  mode: online

job_name: ???
exp_name: ???
description: ???
fold: ???
seed: ???
phase: train # train or test
dry_run: false
debug: false
cleanup: true
final_submission: false
check_only: false
no_eval: false